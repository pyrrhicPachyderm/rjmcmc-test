\documentclass[12pt]{article}
\usepackage{amsmath}

\newcommand*\Norm{\mathrm{N}}
\newcommand*\InvGamma{\mathrm{InvGamma}}

\begin{document}

Here, I examine the problem analytically, using conjugate priors.
Let the data be $\left(\vec{x},\vec{y}\right)$.
Let $x_i \sim \Norm\left(0, \sigma_x^2\right)$ and $y_i \sim \Norm\left(0, \sigma_y^2\right)$.
Let $n$ be the length of $\vec{x}$ and $\vec{y}$.

The two models are:
\begin{align*}
	y_i &= \epsilon_i ,\\
	y_i &= \beta x_i + \epsilon_i ,\\
\end{align*}
where $\epsilon_i \sim \Norm\left(0, \sigma^2\right)$.

In either case, we use the inverse gamma distribution as a conjugate prior for $\sigma^2$:
\[
	\sigma^2 = \InvGamma\left(a_0, b_0\right) .
\]
For the first model, this makes the posterior:
\begin{align*}
	\sigma^2 \mid y_i \sim \InvGamma\left(a_0 + \frac{n}{2}, b_0 + \frac{1}{2} \sum_i y_i^2 \right) ,
\end{align*}
noting that $\sigma^2$ is not conditional on $\vec{x}$.

For the second model, we define the prior for the slope as:
\[
	\beta \mid \sigma^2 \sim \Norm\left(0, \sigma^2 {\lambda_0}^{-1}\right) .
\]
The posterior is then:
\begin{align*}
	\lambda_n &= \lambda_0 + \sum_i x_i^2 \\
	\hat{\beta} &= \frac{\sum_i x_i y_i}{\sum_i x_i^2} \text{ (the maximum likelihood slope)} \\
	\mu_\beta &= \lambda_n^{-1}\left(\hat{\beta} \sum_i x_i^2\right) \\
	&= \lambda_n^{-1} \sum_i x_i y_i \\
	\beta \mid \sigma^2,x_i,y_i &\sim \Norm\left(\mu_\beta, \sigma^2\lambda_n^{-1}\right) \\
	\sigma_2 \mid x_i,y_i &\sim \InvGamma\left(a_0 + \frac{n}{2}, b_0 + \frac{1}{2}\left(y_i^2 + \lambda_n \mu_\beta^2\right)\right) \\
\end{align*}

\end{document}
